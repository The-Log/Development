% Created 2017-09-26 Tue 15:30
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\author{Ankur Mishra}
\date{09/4/2017 - 9/15/2017}
\title{Journal}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.1.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\tableofcontents

\section{RL Notes}
\label{sec-1}
\subsection{Markov Processes}
\label{sec-1-1}
\begin{itemize}
\item Where the environmnent fully observable
\item Almost all RL problems can be characterized as MDPs
\end{itemize}
\subsubsection{Markov Property}
\label{sec-1-1-1}
\begin{itemize}
\item P[S$_{\text{(t+1)}}$ | S$_{\text{t]}}$ = P[S$_{\text{(t+1)}}$| S$_{\text{1}}$, \ldots{}., S$_{\text{t]}}$
\item Future is irrelavent of past, only related to present
\item Given S$_{\text{(t)}}$, you don't need anything else to find to find next state s'
\item Transition Matrix P defines probabilites for all successive states S'
\end{itemize}
\subsubsection{Markov Chains}
\label{sec-1-1-2}
M = \{S, T\}
\begin{itemize}
\item Episodes are random sequences that are sampled.
\item S = State Space
\item T = Transition Probability or the probabililty of entering the next state
\item 
\end{itemize}
\subsection{Markov Reward Process}
\label{sec-1-2}
M = \{S, T, R\}
\begin{itemize}
\item MRP is a tuple of (S is a finite set of states, P is a state of the transitionprobability matrix, Reward Function R, dicount factor $\gamma$)
\item R = E[R$_{\text{(t+1)}}$ | S$_{\text{t}}$ = s]
\end{itemize}
R$_{\text{(t+1)}}$ is the amount of reward we get from state s
\begin{itemize}
\item We care about the cumulative reward
\end{itemize}
\subsubsection{Return (goal)}
\label{sec-1-2-1}
Definition: total discounted reward from time-step t
\begin{itemize}
\item G$_{\text{t}}$ = R$_{\text{(t+1)}}$ + $\gamma$ * (R$_{\text{(t+1)}}$) +  \ldots{}
\item Made finite by the $\gamma$
\item $\gamma$ is going to have to be [0,1]; 0 discounted factor means you only care about present Reward, 1 factor means you care about all of them
\item Discount factor is used because we don't have a perfect model, avoids infinite returns, and animals show a preference for immediate reward
\end{itemize}
\subsubsection{Bellman Equation}
\label{sec-1-2-2}
The Bellman Equation determines value of a state. It is comprised of immediate reward (R$_{\text{(t+1)}}$) and value of next state ($\gamma$*v(S$_{\text{(t+1)}}$))
\begin{itemize}
\item Equation: v(s) = E[G$_{\text{t}}$ | S$_{\text{t}}$ =s] = E[R$_{\text{(t+1)}}$ + $\gamma$ * v(S$_{\text{(t+1)}}$) | S$_{\text{t}}$ = s]
\end{itemize}
It is a linear quation and can be solved. 
\subsection{Markov Decison Process}
\label{sec-1-3}
M = \{S, A, T, R\}
\begin{itemize}
\item MDP is the same as MRP except with the addition of A (the action space)
\end{itemize}
\subsubsection{Policy}
\label{sec-1-3-1}
$\pi$(a|s) = P[A$_{\text{t}}$ = a | S$_{\text{t}}$ = s]
\begin{itemize}
\item A policy defines the behavior of an agent. It picks the actions that get the most reward.
\item 
\end{itemize}
% Emacs 25.1.1 (Org mode 8.2.10)
\end{document}
