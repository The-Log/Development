% Created 2018-03-06 Tue 10:22
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\author{Ankur Mishra}
\date{2/26/18}
\title{Neural Networks}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.2.2 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\tableofcontents

\section{Optimization}
\label{sec-1}
\subsection{Two strategies}
\label{sec-1-1}
\begin{enumerate}
\item Random Search\\
\label{sec-1-1-0-1}
Randomly look through weights and record the W that returns the lowest loss. In a nutshell this is
guess and check, which pretty much sucks, but slightly better than baseline (10\% < 15\%).
\item Gradient Descent\\
\label{sec-1-1-0-2}
Computing the slope accross every single direction; derivative. In multiple dimensions, the gradient is a vector of
partial derivatives. It can be done numerically and analytically. In general, always use analytic gradients, but check if it is right with numeric gradients; also known as
a gradient check. 
\begin{enumerate}
\item Numerically Gradient\\
\label{sec-1-1-0-2-1}
Computing it can be thought as taking really small steps and finding the slope (Difference In Losses/Distance Between Points).\\
Evaluating numerically is approximate and very slow, so don't do it. They are just easy to write.
\item Analytic Gradient\\
\label{sec-1-1-0-2-2}
Taking Derivatives. These are exact and very fast, but tricky to implement.\\
\end{enumerate}
\end{enumerate}
\subsection{Mini-Batch Gradient Descent}
\label{sec-1-2}
Using small sections of training set to compute gradient. This is faster and better for the overall network 
and also creates noise which is better for optimization. Full-Batch will just give a straight line.\\
Common Sizes: 32, 64, 128.
Usually start with high learning rate and decays over time/epochs.
\section{Neural Networks}
\label{sec-2}
\subsection{Backpropagation}
\label{sec-2-1}
Way of computing the influence of every value on a computational graph by recursively using multivariable chain 
rule through the graph. \\ \\
Chain Rule: $dL/dx = dL/dz * dz/dx$ \\ \\
You can break your backprop into other functions and find their derivatives. An example of this is breaking 
$\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$ into the sigmoid function: $\frac{1}{1+e^{-x}}$
\begin{itemize}
\item plus (+) gate distrubutes gradients equally
\item max gate routes gradient to max
\item multiply (*) switches inputs and multiplies each by global gradient, equal inputs are picked arbitrarily
\end{itemize}
If two gradients combine when backpropagating --> add their gradients
Linear Score Function: $f = W*x$ \\
Two Layer Neural Network: $f = W_2 *max(0,W_1*x)$ \\ or Three Layers N-Network $f = W_3*max(W_2*max(0,W_1*x))$ \\
Bigger Networks are more powerful.\\
\subsection{Activation Functions}
\label{sec-2-2}
\subsubsection{Sigmoid Function}
\label{sec-2-2-1}
Equation: $o(x) = \frac{1}{1+e^{-x}}$ \\ Historically the most popular since the implementation has a saturating
firing rate. It squashes a number between 0 to 1.\\
Issues:
\begin{enumerate}
\item Saturated neurons kill gradients \\
\label{sec-2-2-1-1}
Only flows in the active region. If values are relatively high or low, gradients will only come out to be 0. Also drastically different inputs may come out to same outputs.
\item Outputs aren't 0 centered \\
\label{sec-2-2-1-2}
It doesn't converge as nicely. Integral of region comes out to 0.
\item Compute Time is Longer for Exponential Functions
\label{sec-2-2-1-3}
\end{enumerate}
\subsubsection{tanh(x) Function}
\label{sec-2-2-2}
Is zero centered, range is between [-1, 1], and is 0 centered. Still has issue of killing gradients like sigmoid.
\subsection{Rectified Linear Unit (ReLU)}
\label{sec-2-3}
Equation: $f(x) = max(0,x)$ \\ Current standard for activiation functions, as it remedies most of tanh(x) and sigmoids problems, as it
does not saturate (in + region) and is much more efficient. Still has issues:
\begin{enumerate}
\item Not Zero Centered
\label{sec-2-3-0-1}
\item Kills Gradients which x < 0
\label{sec-2-3-0-2}
\item Some initialization results in dead ReLUs
\label{sec-2-3-0-3}
If a neuron is not in activation region, it will die and never update. 
To fix people initialize slightly positive biases like .01; though not always effective.
\subsubsection{Leaky ReLU / Parametric ReLU}
\label{sec-2-3-1}
Leaky ReLU equation $f(x)=max(.01x, x)$. Parametric ReLU equation: $f(x)=max(\alpha x, x)$, where
$\alpha$ is a parameter that can be learned according to the network. These two maintain all the perks of
ReLU and do not die, but still aren't amazing. Also aren't zero centered.
\subsubsection{Exponential Linear Units}
\label{sec-2-3-2}
Exponential ReLU has all the benefits of ReLU, don't die, and closer to zero mean outputs, but compution for
exponential takes time.
\begin{equation}
\text{Equation:}\ f(x) =  
      \begin{cases}
          x, & \text{if}\ x > 0 \\
          \alpha  (exp(x) - 1), & \text{if}\ x \leq 0
      \end{cases}
\end{equation}
\subsection{Image Pre-processing}
\label{sec-2-4}
Commonly pre-processing of images is done by mean centering. This either means to to subtract the mean
value of each pixel by a [32,32,3] array, or to find the per-channel mean, which is subtract the mean from each
pixel's RGB channels.
\subsection{Weight Initialization}
\label{sec-2-5}
Setting weights to 0 will return 0 throughout network. Even .01 returns near zero values over the last few 
layers of a network in both forward and backward pass, which is known as vanishing gradient. Setting weight to 1, will supersaturate network, as all neurons come out as -1 or 1. The solution is Xavier initialization.
\subsubsection{Xavier Initialization}
\label{sec-2-5-1}
W = np.random.randn(fan$_{\text{in}}$, fan$_{\text{out}}$) / np.sqrt(fan$_{\text{in}}$) for tanh(x). This breaks when using ReLU, so use
W = np.random.randn(fan$_{\text{in}}$, fan$_{\text{out}}$) / np.sqrt(fan$_{\text{in}}$ / 2) for ReLU.
\subsection{Batch Normalization}
\label{sec-2-6}
This is to normalize data where you apply this equation to each layer:
$$\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{(Var(x^{(k)}))}}$$ 
Which is a vanilla differentiable function. What it does is it computes the mean of every feature and then divides by it.
$$ y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}$$
After this the function scaled by $\gamma$ and then is shifted by $\beta$, which changes the range if the network wants to. Through learning the network can either learn
to take it out or take advantage of it. \\
The general process of this is first the  mini-batch mean is computed, then its variance. 
Using these two things, the values are normalized and finally are scaled and shifted.
\subsubsection{Perks of Using It:}
\label{sec-2-6-1}
\begin{enumerate}
\item Improves Gradient Flow
\label{sec-2-6-1-1}
\item Allows for High Learning Rates
\label{sec-2-6-1-2}
\item Reduces dependence on strong weight initialization
\label{sec-2-6-1-3}
\item Acts like regularization and slightly reduces need for dropout
\label{sec-2-6-1-4}
\end{enumerate}
\subsection{Debugging Training}
\label{sec-2-7}
\begin{enumerate}
\item Check if loss is reasonable \\
\label{sec-2-7-0-1}
First disable regularization, and check loss. Then increase regularation, and check loss. If you are doing it right, the loss will
also go up. \\ Then check if you can overfit your data with a small portion of your data-set with no regularation, by getting
a loss/cost goes to 0 and accuracy is 100\%.
\item Your Learning Rate Should be Between 1e-3 to 1e-5
\label{sec-2-7-0-2}
\end{enumerate}
\subsection{Parameter Updates}
\label{sec-2-8}
\subsubsection{Simple Gradient Descent}
\label{sec-2-8-1}
Get Batch, Calculate Loss with Forward Pass, then Calculate Gradient with Backward Pass, and then perform parameter update. 
This however is the slowest way of training.
\subsubsection{Momentum}
\label{sec-2-8-2}
A way of fixing this is to use momentum that uses v, which equals $\mu * v - \text{(learning rate)}dx$. A physical representation
of this is to think of it like a ball traveling down a U-shaped ramp. The learning rate is the ball's acceleration, 
and $\muÂ·v - text{(learning rate)}dx$ is the ball's total velocity, which slowing down as the loss should be improving over time. \\
In general this usually overshoots and then converges to the expected loss. The velocity starts at 0.
\subsubsection{Nesterov Accelerated Gradient}
\label{sec-2-8-3}
\subsubsection{AdaGrad}
\label{sec-2-8-4}
\subsubsection{RMSProp}
\label{sec-2-8-5}
\subsubsection{Adam}
\label{sec-2-8-6}
Good for default cases. 
\subsubsection{Hessian and L-BFGS}
\label{sec-2-8-7}
\subsection{Regularization: Drop-out}
\label{sec-2-9}
% Emacs 25.2.2 (Org mode 8.2.10)
\end{document}
